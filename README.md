# LLMs-Powered Product Matching Pipeline

*A multimodal image and text-based product matching system using BERT, DINOv2, and LLaVA.*

 **Motivation:** Inspired by research in vision-language models and large-scale product matching.

# Overview:

This project is a fast, scalable, and efficient product matching system that leverages multimodal inputsâ€”both text and imagesâ€”to perform semantic similarity search. It's optimized using NVIDIA Triton Inference Server and integrates state-of-the-art models like DINOv2, BERT, and LLaVA for rich embeddings and captioning.

---

## Models Used

- **DINOv2** â€“ Generates Image Embeddings  
- **BERT** â€“ Generates text embeddings  
- **LLaVA-OneVision** â€“ Captioning Input Image

---

## Tech Stack

-  **FastAPI** â€“ Backend API  
-  **Gradio** â€“ Web Frontend GUI  
-  **Triton Inference Server** â€“ TensorRT Optimized model deployment  
-  **FAISS** â€“ Vector Database
-  **MongoDB** â€“ Stores metadata (Including Images)  
-  **Docker Compose** â€“ Orchestrates all Containers
---

## Features

- Supports **Image** or **Text** based Semantic Search
- TensorRT-powered inference via **Triton**
- FAISS + MongoDB integration for embeddings & metadata Storage
- Easy-to-use **Gradio UI**
---
## Project Structure

```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ assets
â”‚Â Â  â”œâ”€â”€ workflow.drawio
â”‚Â Â  â””â”€â”€ workflow.drawio.png
â”œâ”€â”€ backend
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ batching
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ base.py
â”‚Â Â  â”œâ”€â”€ core
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ app_state.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config_loader.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ custom_log.py
â”‚Â Â  â”œâ”€â”€ database
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ faiss_db.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ mongo_db.py
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bert_model.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dinov2_model.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ llava_model.py
â”‚Â Â  â”œâ”€â”€ routes
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ api_router.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ image_search.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logger.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ text_search.py
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ img_vector_search.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llava_bert.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llava_runner.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongodb.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ text_vector_search.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ triton_client.py
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ logging.py
â”‚Â Â      â””â”€â”€ rerank.py
â”œâ”€â”€ main.py
â”‚Â 
â”œâ”€â”€ configs
â”‚Â Â  â””â”€â”€ app_config.yaml
â”œâ”€â”€ dataset
â”‚Â Â  â”œâ”€â”€ sample
â”‚Â Â  â”‚Â Â  â””â”€â”€ images
â”‚Â Â  â”œâ”€â”€ deepfashion_loader.py
â”‚Â Â  â””â”€â”€ prepare_metadata.py
â”œâ”€â”€ db
â”‚   â”œâ”€â”€ faiss
â”‚Â Â  â”‚Â Â  Â  â”œâ”€â”€ bert_index.faiss
â”‚   â”‚     â””â”€â”€ dino_index.faiss
â”‚Â Â  â””â”€â”€ mongo
â”‚        â””â”€â”€ metadata.json
â”œâ”€â”€ docker         
â”‚      â”œâ”€â”€ Dockerfile.backend
â”‚      â”œâ”€â”€ Dockerfile.mongo
â”‚      â””â”€â”€ Dockerfile.triton
â”‚
â”œâ”€â”€ frontend
â”‚Â Â  â””â”€â”€â”€ app.py
â”‚
â”œâ”€â”€ engines
â”‚Â Â  â”œâ”€â”€ bert
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ model.plan
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bert.onnx
â”‚Â Â  â”‚Â Â  â””â”€â”€ config.pbtxt
â”‚Â Â  â”œâ”€â”€ dinov2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ model.plan
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.pbtxt
â”‚Â Â  â”‚Â Â  â””â”€â”€ dinov2.onnx
â”‚Â Â  â””â”€â”€ llava_to_bert
â”‚Â Â      â”œâ”€â”€ 1
â”‚Â Â      â”‚Â Â  â””â”€â”€ model.py
â”‚Â Â      â””â”€â”€ config.pbtxt
â”œâ”€â”€ llava_vision
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â””â”€â”€ rank0.engine
â””â”€â”€ llava_vision_encoder
    â”œâ”€â”€ config.json
    â”œâ”€â”€ image_newlines.safetensors
    â””â”€â”€ model.engine

â”œâ”€â”€ scripts   Â  
â”‚    â”œâ”€â”€ bert_onnx_convert.py
â”‚    â”œâ”€â”€ dinov2_onnx_convert.py
â”‚    â”œâ”€â”€ download_build_llava.sh
â”‚    â”œâ”€â”€ test.jpg
â”‚    â””â”€â”€ test_triton_client.py
â”œâ”€â”€ trt_engine       
â”‚     â”œâ”€â”€ __init__.py
â”‚     â”œâ”€â”€ bert_engine.py
â”‚     â”œâ”€â”€ dinov2_engine.py
â”‚     â”œâ”€â”€ llava_engine.py
â”‚     â”œâ”€â”€ llava_utils.py
â”‚     â””â”€â”€ trt_base.py
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements_app.txt
â””â”€â”€ requirements_server.txt
```
---

## Workflow

<img src="assets/workflow.drawio.png" width="600" height="300">
---

## Dataset â€“ DeepFashion

We use a **subset of the DeepFashion dataset** around 2k fashion product images:

- High-resolution product images  
- Product metadata (title, description, category)  
- Fine-grained attributes  
- [DeepFashion Dataset Info](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)
---

## Token Limit Notice:
+ Text input is limited to a maximum of 128 tokens.
+ All input texts are tokenized using **bert-base-uncased**.
+ Inputs longer than 128 tokens will be automatically truncated.

**Note:** For best results, keep prompts short, focused, and within 1â€“2 sentences.
## Triton Platform Specs:
+ GPU: NVIDIA GeForce RTX 4090 (24GB VRAM)
+ CUDA: Version 12.8
+ Driver: 570.124.06
+ CUDA Compiler: 12.6
+ CPU: AMD EPYC 7282, 16-Core

## âš¡ Quick Start Guide

### * Prerequisites:

```bash
- Docker
- NVIDIA GPU (for Triton inference)
- Python 3.8+ (for any local testing)

```

### 1. Clone the Repo
```bash
git clone https://github.com/sudheer-akki/product-matching-pipeline
cd /product-matching-pipeline
```

### 2. Download ONNX Models & Sample Database

```
bash download_models.sh
```
**Note:** Make sure the following files are downloaded and in place:

+ bert_onnx -> models/engines/bert/bert.onnx
+ dino_onnx -> models/engines/dinov2/dino.onnx
+ bert index -> db/faiss/bert.index
+ dino index -> db/faiss/dinov2.index
+ MongoDB Metadata -> db/mongo/metadata.json

### 3. Update configuration data

+ open **configs/app_config.yaml** file

**Note:** Update values if needed

### 4. Start the Application (Docker)

```
docker compose up -d
```
This command will automatically:

+ Build Docker engines for each model if not already present.

+ Start the Triton Inference Server with all necessary backends and dependencies.

### Access the Frontend GUI: http://localhost:7860

## 5. Upload Image
+ Open the GUI in your browser.
+ Upload an image from the **dataset/sample** folder.
+ Check the output results for product matches!

## Demo

Watch a short demo of the Product Matching Pipeline in action:  
ğŸ“º [Click to Watch on Google Drive](https://drive.google.com/file/d/1HMTTXWTheWwC-zN2QKnYVv3Cr8a9dBPi/view?usp=drive_link)

This video includes:
- Launching the web UI
- Uploading a sample product image
- Retrieving visually and semantically similar matches
- Real-time inference using Triton

## Support

If you face any issues using this repository, feel free to [open an Issue](https://github.com/your-username/product-matching-pipeline/issues).  
Iâ€™ll be happy to help troubleshoot and provide guidance.

If you find this project useful, please consider giving it a â­ on GitHub!

## Areas for Improvement

While this system is production-ready and functional, there is **room for improvement** in two key areas:

- **Product Matching Ranking** â€“ Current ranking works well but can be further refined to improve accuracy, especially in visually or semantically similar product groups.
- **Code Optimization** â€“ Certain components (e.g., batching, async inference, reranking logic) can be optimized for performance and efficiency.

These areas are open-ended and can be enhanced depending on specific use cases or deployment needs.


## License

This project is licensed under the [MIT License](LICENSE).
