# Product Matching Pipeline 

Supports both **image** and **text** Inputs.

**Token Limit Notice:**
+ Text input is limited to a maximum of 128 tokens.
+ All input texts are tokenized using **bert-base-uncased**.
+ Inputs longer than 128 tokens will be automatically truncated.

**Note:** For best results, keep prompts short, focused, and within 1â€“2 sentences.

---

## Models Used

- **ğŸ–¼ï¸ DINOv2** â€“ Generates Image Embeddings  
- **ğŸ§  BERT** â€“ Generates text embeddings  
- **ğŸ“¸ LLaVA-OneVision** â€“ Captioning Input Image

---

## Tech Stack

-  **FastAPI** â€“ Backend API  
-  **Gradio** â€“ Web Frontend GUI  
-  **Triton Inference Server** â€“ TensorRT Optimized model deployment  
-  **FAISS** â€“ Vector Database
-  **MongoDB** â€“ Stores metadata (Including Images)  
-  **Docker Compose** â€“ Orchestrates all Containers
---

## Features

- Supports **Image** or **Text** based Semantic Search
- TensorRT-powered inference via **Triton**
- FAISS + MongoDB integration for embeddings & metadata Storage
- Easy-to-use **Gradio UI**
---
## ğŸ“ Project Structure

```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ assets
â”‚Â Â  â”œâ”€â”€ workflow.drawio
â”‚Â Â  â””â”€â”€ workflow.drawio.png
â”œâ”€â”€ backend
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ batching
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ base.py
â”‚Â Â  â”œâ”€â”€ core
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ app_state.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config_loader.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ custom_log.py
â”‚Â Â  â”œâ”€â”€ database
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ faiss_db.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ mongo_db.py
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bert_model.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dinov2_model.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ llava_model.py
â”‚Â Â  â”œâ”€â”€ routes
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ api_router.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ image_search.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logger.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ text_search.py
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ img_vector_search.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llava_bert.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llava_runner.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongodb.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ text_vector_search.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ triton_client.py
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â      â”œâ”€â”€ logging.py
â”‚Â Â      â””â”€â”€ rerank.py
â”œâ”€â”€ main.py
â”‚Â 
â”œâ”€â”€ configs
â”‚Â Â  â””â”€â”€ app_config.yaml
â”œâ”€â”€ dataset
â”‚Â Â  â”œâ”€â”€ sample
â”‚Â Â  â”‚Â Â  â””â”€â”€ images
â”‚Â Â  â”œâ”€â”€ deepfashion_loader.py
â”‚Â Â  â””â”€â”€ prepare_metadata.py
â”œâ”€â”€ db
â”‚   â”œâ”€â”€ faiss
â”‚Â Â  â”‚Â Â  Â  â”œâ”€â”€ bert_index.faiss
â”‚   â”‚     â””â”€â”€ dino_index.faiss
â”‚Â Â  â””â”€â”€ mongo
â”‚        â””â”€â”€ metadata.json
â”œâ”€â”€ docker         
â”‚      â”œâ”€â”€ Dockerfile.backend
â”‚      â”œâ”€â”€ Dockerfile.mongo
â”‚      â””â”€â”€ Dockerfile.triton
â”‚
â”œâ”€â”€ frontend
â”‚Â Â  â””â”€â”€â”€ app.py
â”‚
â”œâ”€â”€ engines
â”‚Â Â  â”œâ”€â”€ bert
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ model.plan
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bert.onnx
â”‚Â Â  â”‚Â Â  â””â”€â”€ config.pbtxt
â”‚Â Â  â”œâ”€â”€ dinov2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ model.plan
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ config.pbtxt
â”‚Â Â  â”‚Â Â  â””â”€â”€ dinov2.onnx
â”‚Â Â  â””â”€â”€ llava_to_bert
â”‚Â Â      â”œâ”€â”€ 1
â”‚Â Â      â”‚Â Â  â””â”€â”€ model.py
â”‚Â Â      â””â”€â”€ config.pbtxt
â”œâ”€â”€ llava_vision
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â””â”€â”€ rank0.engine
â””â”€â”€ llava_vision_encoder
    â”œâ”€â”€ config.json
    â”œâ”€â”€ image_newlines.safetensors
    â””â”€â”€ model.engine

â”œâ”€â”€ scripts   Â  
â”‚    â”œâ”€â”€ bert_onnx_convert.py
â”‚    â”œâ”€â”€ dinov2_onnx_convert.py
â”‚    â”œâ”€â”€ download_build_llava.sh
â”‚    â”œâ”€â”€ test.jpg
â”‚    â””â”€â”€ test_triton_client.py
â”œâ”€â”€ trt_engine       
â”‚     â”œâ”€â”€ __init__.py
â”‚     â”œâ”€â”€ bert_engine.py
â”‚     â”œâ”€â”€ dinov2_engine.py
â”‚     â”œâ”€â”€ llava_engine.py
â”‚     â”œâ”€â”€ llava_utils.py
â”‚     â””â”€â”€ trt_base.py
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements_app.txt
â””â”€â”€ requirements_server.txt
```
---

## Workflow

<img src="assets/workflow.drawio.png" width="600" height="300">
---

## ğŸ“¦ Dataset â€“ DeepFashion

We use a **subset of the DeepFashion dataset** around 2k fashion product images:

- âœ… High-resolution product images  
- âœ… Product metadata (title, description, category)  
- âœ… Fine-grained attributes  
- ğŸ“š [DeepFashion Dataset Info](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)

---
## Triton Platform Specs:
+ GPU: NVIDIA GeForce RTX 4090 (24GB VRAM)
+ CUDA: Version 12.8
+ Driver: 570.124.06
+ CUDA Compiler: 12.6
+ CPU: AMD EPYC 7282, 16-Core

## âš¡ Quick Start 

### 1. Clone the Repo
```bash
git clone https://github.com/sudheer-akki/product-matching-pipeline
cd /product-matching-pipeline
```

### 2. Download ONNX Models & Sample Database

```
bash download_models.sh
```
**Note:** Make sure the following files are downloaded and in place:

+ bert_onnx -> models/engines/bert/bert.onnx
+ dino_onnx -> models/engines/dinov2/dino.onnx
+ bert index -> db/faiss/bert.index
+ dino index -> db/faiss/dinov2.index
+ MongoDB Metadata -> db/mongo/metadata.json

### 3. Update configuration data

+ open **configs/app_config.yaml** file

**Note:** Update values if needed

### 4. Start the Application (Docker)

```
docker compose up -d
```
This command will automatically:

+ Build Docker engines for each model if not already present.

+ Start the Triton Inference Server with all necessary backends and dependencies.

### Access the Frontend GUI: http://localhost:7860

## 5. Upload Image
+ Open the GUI in your browser.
+ Upload an image from the **dataset/sample** folder.
+ Check the output results for product matches!
