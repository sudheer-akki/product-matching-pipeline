#Requerements:
#CUDA 12.6.3 
#Ubuntu 24.04 
#TensorRT  10.7.0.23 
#Tensorrt_llm: 0.16.0

# Stage 1: Base with standard TensorRT backend
FROM nvcr.io/nvidia/tritonserver:24.12-py3 AS trt_base

# Stage 2: TRT-LLM container as primary base
FROM nvcr.io/nvidia/tritonserver:24.12-trtllm-python-py3

# Copy the standard TensorRT backend
COPY --from=trt_base /opt/tritonserver/backends/tensorrt /opt/tritonserver/backends/tensorrt

# Set working directory
WORKDIR /workspace

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git python3-pip && \
    rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements_server.txt .
RUN pip install --no-cache-dir -r requirements_server.txt

# Expose Triton ports
EXPOSE 8000 8001 8002

# Optional: run Triton with default model repo path
# CMD ["tritonserver", "--model-repository=/models"]
